{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5742ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5752f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.query(x)\n",
    "        v = self.query(x)\n",
    "        attention_weights = torch.matmul(q, k.T) / np.sqrt(self.emb_dim)\n",
    "        attention_weights = function.softmax(attention_weights, dim=-1)\n",
    "        attention_values = torch.matmul(attention_weights, v)\n",
    "        return attention_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7ec2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(emb_dim, emb_dim)\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, emb_dim)\n",
    "        self.norm = nn.LayerNorm(emb_dim, emb_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        seq_len, emb_size = query.size()\n",
    "        q = self.query(query).view(seq_len, 2, 5).transpose(0,1)\n",
    "        k = self.key(key).view(seq_len, 2, 5).transpose(0,1)\n",
    "        v = self.value(value).view(seq_len, 2, 5).transpose(0,1)\n",
    "        attention_weights = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention_weights = attention_weights.masked_fill(mask, -1e9)\n",
    "        attention_weights = function.softmax(attention_weights, dim=-1)\n",
    "        attention_values = torch.matmul(attention_weights, v)\n",
    "        attention_values = attention_values.transpose(0, 1).contiguous().view(seq_len, emb_size)\n",
    "        out = self.fc(attention_values) + value\n",
    "        out = self.norm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3297102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, ffn_dim):\n",
    "        super(FFNLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(emb_dim, ffn_dim)\n",
    "        self.linear2 = nn.Linear(ffn_dim, emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = function.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50adda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, num_blocks):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.attention = attention = MultiHeadSelfAttention(emb_dim, num_heads)\n",
    "        self.ffn = FFNLayer(emb_dim, ffn_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # pos_emb = get_pos_emb(x)\n",
    "        # x = x + pos_emb\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = self.attention(x, x, x, None)\n",
    "            x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f159cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ffn_dim, num_blocks):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.attention1 = MultiHeadSelfAttention(emb_dim, num_heads)\n",
    "        self.attention2 = MultiHeadSelfAttention(emb_dim, num_heads)\n",
    "        self.ffn = FFNLayer(emb_dim, ffn_dim)\n",
    "        \n",
    "    def forward(self, x, enc_out, mask=None):\n",
    "        # pos_emb = get_pos_emb(x)\n",
    "        # x = x + pos_emb\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = self.attention1(x, x, x)\n",
    "            x = self.attention2(enc_out, enc_out, x)\n",
    "            x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5581cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 6., 6., 6., 3., 2., 7., 4., 8., 8.],\n",
       "        [7., 5., 2., 3., 1., 8., 6., 9., 5., 2.],\n",
       "        [4., 1., 4., 7., 5., 7., 7., 7., 3., 3.],\n",
       "        [9., 5., 1., 2., 5., 4., 5., 7., 5., 7.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = torch.randint(1, 10, size=(4, 10)).float()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90673342",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(10, 2, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf323f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2201, -0.3998, -0.1858, -0.2635,  0.1850,  0.1228, -0.3708,  0.0804,\n",
       "         -0.1740,  0.2693],\n",
       "        [ 0.2205, -0.3996, -0.1857, -0.2649,  0.1835,  0.1241, -0.3695,  0.0801,\n",
       "         -0.1737,  0.2690],\n",
       "        [ 0.2241, -0.3981, -0.1851, -0.2759,  0.1717,  0.1345, -0.3594,  0.0777,\n",
       "         -0.1715,  0.2671],\n",
       "        [ 0.2198, -0.4043, -0.1846, -0.2533,  0.1984,  0.1122, -0.3773,  0.0848,\n",
       "         -0.1770,  0.2694]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = encoder(embedding, None)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f2a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(10, 2, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986019b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0602, -0.1558,  0.0338, -0.0008,  0.2352,  0.4724,  0.3739, -0.1169,\n",
       "          0.1715,  0.0426],\n",
       "        [ 0.0600, -0.1558,  0.0335, -0.0008,  0.2354,  0.4723,  0.3737, -0.1169,\n",
       "          0.1714,  0.0428],\n",
       "        [ 0.0600, -0.1558,  0.0336, -0.0008,  0.2353,  0.4723,  0.3738, -0.1170,\n",
       "          0.1715,  0.0427],\n",
       "        [ 0.0601, -0.1558,  0.0337, -0.0008,  0.2353,  0.4723,  0.3738, -0.1169,\n",
       "          0.1715,  0.0427]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = decoder(embedding, out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98df10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
